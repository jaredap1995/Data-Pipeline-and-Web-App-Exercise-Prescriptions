{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57675fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dense\n",
    "# from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "\n",
    "\n",
    "def convert_to_int(arr):\n",
    "    try:\n",
    "        return np.array([int(float(elem)) for elem in arr], dtype=int)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "def get_intensity_range(exercise_options_df, intensity):\n",
    "    if intensity.lower() == 'light':\n",
    "        quantile_value = exercise_options_df['VL'].quantile(1/4)\n",
    "        filtered_options = exercise_options_df[exercise_options_df['VL'] <= quantile_value].sort_values(by='VL')\n",
    "        return filtered_options['VL']\n",
    "    elif intensity.lower() == 'moderate':\n",
    "        quantile_value_lower = exercise_options_df['VL'].quantile(1/4)\n",
    "        quantile_value_upper = exercise_options_df['VL'].quantile(3/4)\n",
    "        filtered_options = exercise_options_df[(exercise_options_df['VL'] > quantile_value_lower) & \n",
    "                                               (exercise_options_df['VL'] <= quantile_value_upper)].sort_values(by='VL')\n",
    "        return filtered_options['VL']\n",
    "    elif intensity.lower() == 'heavy':\n",
    "        quantile_value = exercise_options_df['VL'].quantile(3/4)\n",
    "        filtered_options = exercise_options_df[exercise_options_df['VL'] > quantile_value].sort_values(by='VL')\n",
    "        return filtered_options['VL']\n",
    "    else:\n",
    "        raise ValueError(\"Invalid intensity value\")\n",
    "\n",
    "\n",
    "def load_prepare_data(conn):\n",
    "    cursor=conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            c.name AS client_name, \n",
    "            e.exercise AS exercise_name, \n",
    "            td.weight, \n",
    "            td.sets, \n",
    "            td.reps\n",
    "        FROM training_data td\n",
    "        LEFT JOIN client c ON td.client_id = c.id\n",
    "        LEFT JOIN exercises e ON td.exercise_id = e.id;\n",
    "        \"\"\")\n",
    "    all_workout_data = cursor.fetchall()\n",
    "    all_workout_data = np.asarray(all_workout_data)\n",
    "\n",
    "    int_var=[]\n",
    "    variables=all_workout_data[:,2:]\n",
    "    for i, arr in enumerate(variables):\n",
    "        converted_arr = convert_to_int(arr)\n",
    "        if converted_arr is not None:\n",
    "            int_var.append(converted_arr)\n",
    "\n",
    "    df=pd.DataFrame(all_workout_data)\n",
    "    df=df.drop(columns=[2,3,4])\n",
    "\n",
    "    variables=pd.DataFrame(int_var)\n",
    "    variables.columns=['Weight', 'Sets', 'Reps']\n",
    "    df=pd.concat([df, variables], axis=1)\n",
    "\n",
    "    df['VL'] = df['Weight'].mul(df['Sets']).mul(df['Reps'])\n",
    "    df.columns=['Name', 'Exercise', 'Weight', 'Sets', 'Reps', 'VL']\n",
    "\n",
    "    volume_loads=df['VL']\n",
    "    exercises=df['Exercise']\n",
    "    scaled_VL=MinMaxScaler().fit_transform(df['VL'].to_numpy().reshape(-1,1))\n",
    "\n",
    "    #indexes of I'Y'T's exercises that mess up embeddings later on. Found in accompanying Jupyter and manually removed\n",
    "    non_101_shape=[1401,1466,1490,1589,1608,1730,1777,1822,1846,1894,1899,1907,1992,2035,\n",
    "                    2044,2094,2101,2211]\n",
    "    \n",
    "    exercises[non_101_shape]='eyes, whys, and tees'\n",
    "\n",
    "    return df, volume_loads, exercises, scaled_VL\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def __iter__(self):\n",
    "        for document in self.documents:\n",
    "            # assume there's one document per element in the list, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(document)\n",
    "\n",
    "def corpus_build(exercises):\n",
    "    tokens=[token for token in MyCorpus(exercises)]\n",
    "    corpus_instance = MyCorpus(exercises)\n",
    "    model = gensim.models.Word2Vec(vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Build vocabulary from the corpus\n",
    "    model.build_vocab(corpus_instance)\n",
    "\n",
    "    # Train the model on the corpus\n",
    "    model.train(corpus_instance, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "    exercise_vectors = []\n",
    "    for exercise in tokens:\n",
    "        exercise_vector = np.mean([model.wv[word] for word in exercise], axis=0)\n",
    "        exercise_vectors.append(exercise_vector)\n",
    "\n",
    "    return exercise_vectors\n",
    "\n",
    "def sanitizie_inputs(exercise_vectors, scaled_VL):\n",
    "    # Combine exercise vectors with volume loads\n",
    "    input_data = []\n",
    "    for exercise_vector, volume_load_normalized in zip(exercise_vectors, scaled_VL):\n",
    "        combined = np.hstack((exercise_vector, volume_load_normalized))\n",
    "        input_data.append(combined)\n",
    "    input_data = np.array(input_data)\n",
    "\n",
    "    input_data=np.array(input_data, dtype=np.float32)\n",
    "\n",
    "    return input_data\n",
    "\n",
    "def load_model_make_predictions(input_data):\n",
    "    input_size = input_data[0].shape[0]\n",
    "    encoding_dim = 32\n",
    "\n",
    "    # Provided for reference...model already trained and called upon in app.py\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_size,))\n",
    "    encoded = tf.keras.layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoded = tf.keras.layers.Dense(input_size, activation='sigmoid')(encoded)\n",
    "\n",
    "    \n",
    "    # autoencoder = Model(input_layer, decoded)\n",
    "    # autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    # autoencoder.fit(input_data, input_data, epochs=100, batch_size=32, shuffle=True)\n",
    "\n",
    "    autoencoder= tf.keras.models.load_model('autoencoder_exercise_selector.h5')\n",
    "    encoder=tf.keras.models.Model(input_layer, encoded)\n",
    "    encoded_exercises = encoder.predict(input_data)\n",
    "    similarity_matrix = cosine_similarity(encoded_exercises)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def find_similar_exercises(exercise_index, exercises, similarity_matrix, top_n):\n",
    "    similarity_scores = similarity_matrix[exercise_index]\n",
    "\n",
    "    # Sort the similarity scores and get the indices\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "\n",
    "    # Exclude exercises with the same string value as the original exercise\n",
    "    original_exercise = exercises[exercise_index]\n",
    "    unique_indices = [idx for idx in sorted_indices if exercises[idx] != original_exercise]\n",
    "\n",
    "    # Find top_n unique exercises\n",
    "    top_n_indices = []\n",
    "    unique_exercises = set()\n",
    "    for idx in unique_indices:\n",
    "        if exercises[idx] not in unique_exercises:\n",
    "            unique_exercises.add(exercises[idx])\n",
    "            top_n_indices.append(idx)\n",
    "            if len(top_n_indices) >= top_n:\n",
    "                break\n",
    "\n",
    "    # Return the indices of the top n most similar unique exercises\n",
    "    return top_n_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1cd4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_selector(conn):\n",
    "    if 'exercise_selector' not in st.session_state:\n",
    "        st.session_state.exercise_selector = False\n",
    "\n",
    "    tokenizer=tf.keras.preprocessing.text.Tokenizer\n",
    "    pad_sequences=tf.keras.preprocessing.sequence.pad_sequences\n",
    "    input_tokenizer = tokenizer(char_level=False, filters='', lower=False)\n",
    "\n",
    "\n",
    "    df, volume_loads, exercises, scaled_VL = load_prepare_data(conn)\n",
    "    exercise_vectors = corpus_build(exercises)\n",
    "    input_data = sanitizie_inputs(exercise_vectors, scaled_VL)\n",
    "    similarity_matrix = load_model_make_predictions(input_data)\n",
    "    exercise=st.multiselect('Select exercises', exercises.unique())\n",
    "    workout_length=st.slider('Select number of exercises', 1, 15)\n",
    "    intensities=['Light', 'Moderate', 'Heavy']\n",
    "    intensity=st.selectbox('Select intensity', intensities)\n",
    "    provide_suggestions=st.button('Provide suggestions')\n",
    "    if provide_suggestions or st.session_state.exercise_selector:\n",
    "        st.session_state.exercise_selector = True\n",
    "        try:\n",
    "            exercise_options=df[df['Exercise']==exercise[0]]\n",
    "            VL_range=get_intensity_range(exercise_options, intensity)\n",
    "            exercise_index=random.choice(VL_range.index)\n",
    "            similar_exercise_indices = find_similar_exercises(exercise_index, exercises, similarity_matrix, top_n=workout_length)\n",
    "            semantic_vl_exercises_list=exercises[similar_exercise_indices]\n",
    "            # Load the trained model from a file and tokeinze for regression\n",
    "\n",
    "            loaded_regressor = joblib.load('DTR_exercise_variables.joblib')\n",
    "            input_tokenizer.fit_on_texts(semantic_vl_exercises_list)\n",
    "            token_exercise=input_tokenizer.texts_to_sequences(semantic_vl_exercises_list)\n",
    "            token_exercise=np.asarray(token_exercise)\n",
    "            token_exercise=pad_sequences(token_exercise, maxlen=6, padding='pre')\n",
    "\n",
    "            # Make predictions\n",
    "            predicted_output = loaded_regressor.predict(token_exercise)\n",
    "            predicted_output=predicted_output.astype(int)\n",
    "            df=pd.DataFrame({'Exercise': semantic_vl_exercises_list,\n",
    "                    'Weight': predicted_output[:,0],\n",
    "                    'Sets': predicted_output[:,1],\n",
    "                    'Reps': predicted_output[:,2]})\n",
    "            st.experimental_data_editor(df)\n",
    "        except IndexError as e:\n",
    "            if \"list index\" in str(e):\n",
    "                st.error(\"Please select an exercise\")\n",
    "                st.stop()\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b9d030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
