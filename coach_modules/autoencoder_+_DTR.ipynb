{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42226b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dense\n",
    "# from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "def convert_to_int(arr):\n",
    "    try:\n",
    "        return np.array([int(float(elem)) for elem in arr], dtype=int)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "def get_intensity_range(exercise_options_df, intensity):\n",
    "    if intensity.lower() == 'light':\n",
    "        quantile_value = exercise_options_df['VL'].quantile(1/4)\n",
    "        filtered_options = exercise_options_df[exercise_options_df['VL'] <= quantile_value].sort_values(by='VL')\n",
    "        return filtered_options['VL']\n",
    "    elif intensity.lower() == 'moderate':\n",
    "        quantile_value_lower = exercise_options_df['VL'].quantile(1/4)\n",
    "        quantile_value_upper = exercise_options_df['VL'].quantile(3/4)\n",
    "        filtered_options = exercise_options_df[(exercise_options_df['VL'] > quantile_value_lower) & \n",
    "                                               (exercise_options_df['VL'] <= quantile_value_upper)].sort_values(by='VL')\n",
    "        return filtered_options['VL']\n",
    "    elif intensity.lower() == 'heavy':\n",
    "        quantile_value = exercise_options_df['VL'].quantile(3/4)\n",
    "        filtered_options = exercise_options_df[exercise_options_df['VL'] > quantile_value].sort_values(by='VL')\n",
    "        return filtered_options['VL']\n",
    "    else:\n",
    "        raise ValueError(\"Invalid intensity value\")\n",
    "\n",
    "\n",
    "def load_prepare_data(conn):\n",
    "    cursor=conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \n",
    "            c.name AS client_name, \n",
    "            e.exercise AS exercise_name, \n",
    "            td.weight, \n",
    "            td.sets, \n",
    "            td.reps\n",
    "        FROM training_data td\n",
    "        LEFT JOIN client c ON td.client_id = c.id\n",
    "        LEFT JOIN exercises e ON td.exercise_id = e.id;\n",
    "        \"\"\")\n",
    "    all_workout_data = cursor.fetchall()\n",
    "    all_workout_data = np.asarray(all_workout_data)\n",
    "\n",
    "    int_var=[]\n",
    "    variables=all_workout_data[:,2:]\n",
    "    for i, arr in enumerate(variables):\n",
    "        converted_arr = convert_to_int(arr)\n",
    "        if converted_arr is not None:\n",
    "            int_var.append(converted_arr)\n",
    "\n",
    "    df=pd.DataFrame(all_workout_data)\n",
    "    df=df.drop(columns=[2,3,4])\n",
    "\n",
    "    variables=pd.DataFrame(int_var)\n",
    "    variables.columns=['Weight', 'Sets', 'Reps']\n",
    "    df=pd.concat([df, variables], axis=1)\n",
    "\n",
    "    df['VL'] = df['Weight'].mul(df['Sets']).mul(df['Reps'])\n",
    "    df.columns=['Name', 'Exercise', 'Weight', 'Sets', 'Reps', 'VL']\n",
    "\n",
    "    volume_loads=df['VL']\n",
    "    exercises=df['Exercise']\n",
    "    scaled_VL=MinMaxScaler().fit_transform(df['VL'].to_numpy().reshape(-1,1))\n",
    "\n",
    "    #indexes of I'Y'T's exercises that mess up embeddings later on. Found in accompanying Jupyter and manually removed\n",
    "    non_101_shape=[1401,1466,1490,1589,1608,1730,1777,1822,1846,1894,1899,1907,1992,2035,\n",
    "                    2044,2094,2101,2211]\n",
    "    \n",
    "    exercises[non_101_shape]='eyes, whys, and tees'\n",
    "\n",
    "    return df, volume_loads, exercises, scaled_VL\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def __iter__(self):\n",
    "        for document in self.documents:\n",
    "            # assume there's one document per element in the list, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(document)\n",
    "\n",
    "def corpus_build(exercises):\n",
    "    tokens=[token for token in MyCorpus(exercises)]\n",
    "    corpus_instance = MyCorpus(exercises)\n",
    "    model = gensim.models.Word2Vec(vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Build vocabulary from the corpus\n",
    "    model.build_vocab(corpus_instance)\n",
    "\n",
    "    # Train the model on the corpus\n",
    "    model.train(corpus_instance, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "    exercise_vectors = []\n",
    "    for exercise in tokens:\n",
    "        exercise_vector = np.mean([model.wv[word] for word in exercise], axis=0)\n",
    "        exercise_vectors.append(exercise_vector)\n",
    "\n",
    "    return exercise_vectors\n",
    "\n",
    "def sanitizie_inputs(exercise_vectors, scaled_VL):\n",
    "    # Combine exercise vectors with volume loads\n",
    "    input_data = []\n",
    "    for exercise_vector, volume_load_normalized in zip(exercise_vectors, scaled_VL):\n",
    "        combined = np.hstack((exercise_vector, volume_load_normalized))\n",
    "        input_data.append(combined)\n",
    "    input_data = np.array(input_data)\n",
    "\n",
    "    input_data=np.array(input_data, dtype=np.float32)\n",
    "\n",
    "    return input_data\n",
    "\n",
    "def load_model_make_predictions(input_data):\n",
    "    input_size = input_data[0].shape[0]\n",
    "    encoding_dim = 32\n",
    "\n",
    "    # Provided for reference...model already trained and called upon in app.py\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_size,))\n",
    "    encoded = tf.keras.layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoded = tf.keras.layers.Dense(input_size, activation='sigmoid')(encoded)\n",
    "\n",
    "    \n",
    "    # autoencoder = Model(input_layer, decoded)\n",
    "    # autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    # autoencoder.fit(input_data, input_data, epochs=100, batch_size=32, shuffle=True)\n",
    "\n",
    "    autoencoder= tf.keras.models.load_model('autoencoder_exercise_selector.h5')\n",
    "    encoder=tf.keras.models.Model(input_layer, encoded)\n",
    "    encoded_exercises = encoder.predict(input_data)\n",
    "    similarity_matrix = cosine_similarity(encoded_exercises)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def find_similar_exercises(exercise_index, exercises, similarity_matrix, top_n):\n",
    "    similarity_scores = similarity_matrix[exercise_index]\n",
    "\n",
    "    # Sort the similarity scores and get the indices\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "\n",
    "    # Exclude exercises with the same string value as the original exercise\n",
    "    original_exercise = exercises[exercise_index]\n",
    "    unique_indices = [idx for idx in sorted_indices if exercises[idx] != original_exercise]\n",
    "\n",
    "    # Find top_n unique exercises\n",
    "    top_n_indices = []\n",
    "    unique_exercises = set()\n",
    "    for idx in unique_indices:\n",
    "        if exercises[idx] not in unique_exercises:\n",
    "            unique_exercises.add(exercises[idx])\n",
    "            top_n_indices.append(idx)\n",
    "            if len(top_n_indices) >= top_n:\n",
    "                break\n",
    "\n",
    "    # Return the indices of the top n most similar unique exercises\n",
    "    return top_n_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87b8eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities=['Light', 'Moderate', 'Heavy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "250dbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_selector(conn, workout_length, exercise, intensity):\n",
    "\n",
    "    tokenizer=tf.keras.preprocessing.text.Tokenizer\n",
    "    pad_sequences=tf.keras.preprocessing.sequence.pad_sequences\n",
    "    input_tokenizer = tokenizer(char_level=False, filters='', lower=False)\n",
    "\n",
    "\n",
    "    df, volume_loads, exercises, scaled_VL = load_prepare_data(conn)\n",
    "    exercise_vectors = corpus_build(exercises)\n",
    "    input_data = sanitizie_inputs(exercise_vectors, scaled_VL)\n",
    "    similarity_matrix = load_model_make_predictions(input_data)\n",
    "    exercise_options=df[df['Exercise']==exercise[0]]\n",
    "    VL_range=get_intensity_range(exercise_options, intensity)\n",
    "    exercise_index=random.choice(VL_range.index)\n",
    "    similar_exercise_indices = find_similar_exercises(exercise_index, exercises, similarity_matrix, top_n=workout_length)\n",
    "    semantic_vl_exercises_list=exercises[similar_exercise_indices]\n",
    "    # Load the trained model from a file and tokeinze for regression\n",
    "\n",
    "    loaded_regressor = joblib.load('DTR_exercise_variables.joblib')\n",
    "    input_tokenizer.fit_on_texts(semantic_vl_exercises_list)\n",
    "    token_exercise=input_tokenizer.texts_to_sequences(semantic_vl_exercises_list)\n",
    "    token_exercise=np.asarray(token_exercise)\n",
    "    token_exercise=pad_sequences(token_exercise, maxlen=6, padding='pre')\n",
    "\n",
    "    # Make predictions\n",
    "    predicted_output = loaded_regressor.predict(token_exercise)\n",
    "#     predicted_output=predicted_output.astype(int)\n",
    "    df=pd.DataFrame({'Exercise': semantic_vl_exercises_list,\n",
    "            'Weight': predicted_output[:,0],\n",
    "            'Sets': predicted_output[:,1],\n",
    "            'Reps': predicted_output[:,2]})\n",
    "\n",
    "    return df, loaded_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61de46ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/c3g7vgxd10n0mdl2_flcjj480000gn/T/ipykernel_42190/2747227120.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exercises[non_101_shape]='eyes, whys, and tees'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/c3g7vgxd10n0mdl2_flcjj480000gn/T/ipykernel_42190/2313931632.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  token_exercise=np.asarray(token_exercise)\n"
     ]
    }
   ],
   "source": [
    "conn=psycopg2.connect(,\n",
    ")\n",
    "\n",
    "test, regressor=exercise_selector(conn, 7, ['Mountain Climbers'], 'light')\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f09b9fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exercise</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Sets</th>\n",
       "      <th>Reps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>Shoulder Internal Rotation</td>\n",
       "      <td>10.379310</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.517241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>Shoulder External Rotation</td>\n",
       "      <td>10.379310</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.517241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>Step Ups</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>DB Incline Bench Press</td>\n",
       "      <td>10.379310</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.517241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>DB OH Press</td>\n",
       "      <td>10.379310</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.517241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>BB OH Press</td>\n",
       "      <td>10.379310</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.517241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>SA OH Banded Static Lunge</td>\n",
       "      <td>10.384615</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.615385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Exercise     Weight  Sets       Reps\n",
       "1757  Shoulder Internal Rotation  10.379310   3.0   9.517241\n",
       "1800  Shoulder External Rotation  10.379310   3.0   9.517241\n",
       "1937                    Step Ups  30.000000   3.0  10.000000\n",
       "2464      DB Incline Bench Press  10.379310   3.0   9.517241\n",
       "1915                 DB OH Press  10.379310   3.0   9.517241\n",
       "2185                 BB OH Press  10.379310   3.0   9.517241\n",
       "1347   SA OH Banded Static Lunge  10.384615   3.0  14.615385"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f42db1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
